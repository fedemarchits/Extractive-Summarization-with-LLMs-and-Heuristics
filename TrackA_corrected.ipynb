{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CpN6IV-eNIEg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU3bmgctWXyJ",
        "outputId": "1f857e45-c65f-4625-aec2-8f5f21586fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai transformers datasets rouge_score nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-47GCvibWsyy"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from rouge_score import rouge_scorer\n",
        "import re\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-qAks57eWvk-"
      },
      "outputs": [],
      "source": [
        "# # Load dataset in exctractive mode\n",
        "# from datasets import load_dataset\n",
        "# dataset = load_dataset(\"sobamchan/aclsum\", \"extractive\")[\"test\"]\n",
        "# print(dataset[0].keys())\n",
        "\n",
        "# model_name = \"Qwen/Qwen3-4B\"\n",
        "\n",
        "# # Replace model name for Qwen or LLaMA\n",
        "# summarizer = pipeline(\"text-generation\", model=model_name)\n",
        "\n",
        "# # replace tokenizer as needed as well\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# gen = pipeline(\"text-generation\", model=model_name, tokenizer=tokenizer, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset in extractive mode\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"sobamchan/aclsum\", \"extractive\")[\"test\"]\n",
        "print(dataset[0].keys())\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"Qwen/Qwen3-4B\" # Use the chat-tuned model\n",
        "\n",
        "# Load the tokenizer and model with trust_remote_code\n",
        "# This is the correct and most reliable way\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")\n",
        "\n",
        "# You can now create a single, correct pipeline if you want to use it\n",
        "# or use the model.generate() method directly as we discussed.\n",
        "summarizer = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "bf3da6d406db453f913ab5076d62a7c0",
            "e9075aed72864d2a93988e1a58cf29b2",
            "b3cf12bd9c0b428c9fd82df2d4b095bb",
            "d74258f39d1d4d8f8fdc6b0e460366c7",
            "f1c95ae78c6743e784bce69dfceb68ed",
            "10b1fc4b5e33413cb9a92403d2f4ab21",
            "d5a1955c2bc141ff90e82385610ddc5f",
            "59d9c4ad53c34f5eb091ca65bfe4cbb2",
            "aa0122927b394e2c88dd25d35ced70bc",
            "c300e4991c094f3ea577c5ba439a8e12",
            "fafe44f7e4e04b8ba92c5cd23ada09ce"
          ]
        },
        "id": "LmD6qDBhMVBI",
        "outputId": "20bbed4d-97e9-4cd5-ef38-3936f08aae04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'source_sentences', 'challenge_sentences', 'approach_sentences', 'outcome_sentences', 'challenge_labels', 'approach_labels', 'outcome_labels'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf3da6d406db453f913ab5076d62a7c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyRs1BmJnpXq",
        "outputId": "ca84d972-06f1-4127-9f1a-873cac0229f6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'source_sentences', 'challenge_sentences', 'approach_sentences', 'outcome_sentences', 'challenge_labels', 'approach_labels', 'outcome_labels'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['challenge_labels'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjDlGkgOnrvs",
        "outputId": "fbf58fec-3e34-493f-85a7-5bf25485ca92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocess into sentence label pair\n",
        "\n",
        "This step is necessary as it will allow us to match the sentence itself with the label. This means that for every possible label (challenge, approach, outcome) we will store the sentences with their proper label in order to be able to evaluate the outcome accordingly.\n",
        "\n",
        "This means that every phrase in the source_sentences section of the dataset will contain a label as well as the aspect. So, the first sentence might be assigned label 1 for aspect challenge and label 0 for aspects outcome and approach."
      ],
      "metadata": {
        "id": "7Bn1qK3zxoRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_aspect_data(example, aspect):\n",
        "    return [\n",
        "        {\"sentence\": sent, \"label\": lab, \"aspect\": aspect}\n",
        "        for sent, lab in zip(example[\"source_sentences\"], example[f\"{aspect}_labels\"])\n",
        "    ]\n",
        "\n",
        "sample = prepare_aspect_data(dataset[0], \"challenge\")\n",
        "print(sample[:3])"
      ],
      "metadata": {
        "id": "8lGlxm5LzP48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8505772e-90b9-4798-97c0-d8a5f5bab698"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'sentence': 'Handling terminology is an important matter in a translation workflow .', 'label': 1, 'aspect': 'challenge'}, {'sentence': 'However , current Machine Translation ( MT ) systems do not yet propose anything proactive upon tools which assist in managing terminological databases .', 'label': 1, 'aspect': 'challenge'}, {'sentence': 'In this work , we investigate several enhancements to analogical learning and test our implementation on translating medical terms .', 'label': 0, 'aspect': 'challenge'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_1 = prepare_aspect_data(dataset[0], \"outcome\")\n",
        "print(sample_1[:3])"
      ],
      "metadata": {
        "id": "CQqCLrUS0GZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1433fe-b85e-449c-ed67-cad8c57f06b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'sentence': 'Handling terminology is an important matter in a translation workflow .', 'label': 0, 'aspect': 'outcome'}, {'sentence': 'However , current Machine Translation ( MT ) systems do not yet propose anything proactive upon tools which assist in managing terminological databases .', 'label': 0, 'aspect': 'outcome'}, {'sentence': 'In this work , we investigate several enhancements to analogical learning and test our implementation on translating medical terms .', 'label': 0, 'aspect': 'outcome'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2 = prepare_aspect_data(dataset[0], \"approach\")\n",
        "print(sample_2[:3])"
      ],
      "metadata": {
        "id": "aA6C_6M00LFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb2f029-9796-4691-e3d3-014873eab9fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'sentence': 'Handling terminology is an important matter in a translation workflow .', 'label': 0, 'aspect': 'approach'}, {'sentence': 'However , current Machine Translation ( MT ) systems do not yet propose anything proactive upon tools which assist in managing terminological databases .', 'label': 0, 'aspect': 'approach'}, {'sentence': 'In this work , we investigate several enhancements to analogical learning and test our implementation on translating medical terms .', 'label': 1, 'aspect': 'approach'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply this to the whole dataset we can use the following code snippet"
      ],
      "metadata": {
        "id": "d2H6NGZm4Wif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_triplets(example):\n",
        "    aspects = [\"challenge\", \"approach\", \"outcome\"]\n",
        "    # collect labels for each aspect\n",
        "    aspect_labels = {a: [lab for _, lab in zip(example[\"source_sentences\"], example[f\"{a}_labels\"])]\n",
        "                     for a in aspects}\n",
        "    # combine into triplets\n",
        "    triplets = []\n",
        "    for i, sent in enumerate(example[\"source_sentences\"]):\n",
        "        triplet = [aspect_labels[\"challenge\"][i],\n",
        "                   aspect_labels[\"approach\"][i],\n",
        "                   aspect_labels[\"outcome\"][i]]\n",
        "        triplets.append(triplet)\n",
        "    example[\"triplets\"] = triplets\n",
        "    return example\n",
        "\n",
        "labeled_dataset = dataset.map(add_triplets)\n",
        "\n",
        "# quick peek\n",
        "print(labeled_dataset[0][\"source_sentences\"][:10])\n",
        "print(labeled_dataset[0][\"triplets\"][:10])\n"
      ],
      "metadata": {
        "id": "sF7sNi5v4aRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484afa23-fe5f-4499-ceb7-d24ea61f5cee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Handling terminology is an important matter in a translation workflow .', 'However , current Machine Translation ( MT ) systems do not yet propose anything proactive upon tools which assist in managing terminological databases .', 'In this work , we investigate several enhancements to analogical learning and test our implementation on translating medical terms .', 'We show that the analogical engine works equally well when translating from and into a morphologically rich language , or when dealing with language pairs written in different scripts .', 'Combining it with a phrasebased statistical engine leads to significant improvements .', 'If machine translation is to meet commercial needs , it must offer a sensible approach to translating terms .', 'Currently , MT systems offer at best database management tools which allow a human ( typically a translator , a terminologist or even the vendor of the system ) to specify bilingual terminological entries .', 'More advanced tools are meant to identify inconsistencies in terminological translations and might prove useful in controlledlanguage situations ( Itagaki et al . , 2007 ) .', 'One approach to translate terms consists in using a domain-specific parallel corpus with standard alignment techniques ( Brown et al . , 1993 ) to mine new translations .', 'Massive amounts of parallel data are certainly available in several pairs of languages for domains such as parliament debates or the like .']\n",
            "[[1, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 1], [0, 0, 0], [1, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Function: expand one document into list of sentence dicts\n",
        "def expand_doc(example):\n",
        "    return {\n",
        "        \"sentences\": [\n",
        "            {\n",
        "                \"sentence\": sent,\n",
        "                \"label_ch\": int(ch),\n",
        "                \"label_ap\": int(ap),\n",
        "                \"label_oc\": int(oc),\n",
        "            }\n",
        "            for sent, ch, ap, oc in zip(\n",
        "                example[\"source_sentences\"],\n",
        "                example[\"challenge_labels\"],\n",
        "                example[\"approach_labels\"],\n",
        "                example[\"outcome_labels\"]\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        "\n",
        "expanded = dataset.map(expand_doc)\n",
        "\n",
        "print(expanded[1][\"sentences\"][:3])\n",
        "print(f'check length: {len(expanded[1][\"sentences\"])} {len(dataset[1][\"source_sentences\"])}')\n"
      ],
      "metadata": {
        "id": "ziIkMSnM6mL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be895a4f-8510-4948-80d3-a612c0cde2a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label_ap': 0, 'label_ch': 1, 'label_oc': 0, 'sentence': 'Reasoning about implied relationships ( e.g. paraphrastic , common sense , encyclopedic ) between pairs of words is crucial for many cross-sentence inference problems .'}, {'label_ap': 1, 'label_ch': 0, 'label_oc': 0, 'sentence': 'This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships .'}, {'label_ap': 1, 'label_ch': 0, 'label_oc': 0, 'sentence': 'Our pairwise embeddings are computed as a compositional function on word representations , which is learned by maximizing the pointwise mutual information ( PMI ) with the contexts in which the two words cooccur .'}]\n",
            "check length: 32 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prompting techniques"
      ],
      "metadata": {
        "id": "QIuFTkim7O17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the vanilla prompt has only the task to select the most important sentences regardless of the aspect"
      ],
      "metadata": {
        "id": "TCd2Z2_VR2q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_vanilla_prompt(sentences):\n",
        "    \"\"\"\n",
        "    Generates a simple, general-purpose vanilla prompt for extractive summarization.\n",
        "    \"\"\"\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select the most important sentences from the document.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Rules:\n",
        "- Select ONLY the most important sentences.\n",
        "- If no sentences are important, return an empty list.\n",
        "- Indices are 1-based.\n",
        "- Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "P0-yZ4b7R2DZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this second vanilla prompt instead selects the most important phrases based on the aspect, so which are the most important phrases connected to challenge, approach and outcome"
      ],
      "metadata": {
        "id": "e4VVn0iFR-q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vanilla prompt\n",
        "# needs to be called three times on the same phrase to understand the three aspects\n",
        "def vanilla_prompt(sentences, target_label):\n",
        "    # target_label ∈ {\"challenge\",\"approach\",\"outcome\"}\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select sentences that express the \"{target_label}\" aspect of the document.\n",
        "\n",
        "Aspect definitions:\n",
        "- challenge: problem, gap, limitation, unmet need, difficulty/motivation.\n",
        "- approach: method, model, system, algorithm, dataset design, procedure.\n",
        "- outcome: results, findings, improvements, metrics, performance, impact.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Rules:\n",
        "- Select ONLY sentences that primarily express the \"{target_label}\" aspect.\n",
        "- If none match, return an empty list.\n",
        "- Indices are 1-based.\n",
        "- Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''\n"
      ],
      "metadata": {
        "id": "9juXCfSvAm9h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Least to most** prompting technique implemented both in its aspect-based version and in the simple version. Ask the model to identify the overall purpose of the document before returning either the aspect-based sentences or the overall most important sentences."
      ],
      "metadata": {
        "id": "Ynts4oQ7S3PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def least_to_most_prompt(sentences, target_label):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select sentences that express the \"{target_label}\" aspect of the document.\n",
        "\n",
        "Aspect definitions:\n",
        "- challenge: problem, gap, limitation, unmet need, difficulty/motivation.\n",
        "- approach: method, model, system, algorithm, dataset design, procedure.\n",
        "- outcome: results, findings, improvements, metrics, performance, impact.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "First, consider the overall purpose of the document and how each sentence contributes to it.\n",
        "Then, from that understanding, select ONLY sentences that primarily express the \"{target_label}\" aspect.\n",
        "If none match, return an empty list.\n",
        "Indices are 1-based.\n",
        "Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "t4I4Zb9nS0zT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def least_to_most_simple_prompt(sentences):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select the most important sentences from the document.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "First, identify the main topics and key arguments of the document.\n",
        "Then, select ONLY the sentences that directly relate to those topics.\n",
        "If no sentences are important, return an empty list.\n",
        "Indices are 1-based.\n",
        "Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "sqN6CcD9S-cK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tool-augmented prompting**. Here the model is instructed to use a \"tool\" or internal function to aid its reasoning. The tool is not a real external program but a conceptual instruction within the prompt itself.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJ5rWzyhTP50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tool_augmented_prompt(sentences, target_label):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select sentences that express the \"{target_label}\" aspect of the document.\n",
        "\n",
        "Aspect definitions:\n",
        "- challenge: problem, gap, limitation, unmet need, difficulty/motivation.\n",
        "- approach: method, model, system, algorithm, dataset design, procedure.\n",
        "- outcome: results, findings, improvements, metrics, performance, impact.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Instructions:\n",
        "1. For each sentence, use the internal `check_aspect(sentence, aspect)` tool.\n",
        "2. The tool's output is 'match' if the sentence primarily describes the \"{target_label}\" aspect, otherwise it is 'no_match'.\n",
        "3. List the sentences that result in a 'match'.\n",
        "4. If no sentences match, return an empty list.\n",
        "5. Indices are 1-based.\n",
        "6. Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "udvUBxJdTBwE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tool_augmented_simple_prompt(sentences):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select the most important sentences from the document.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Instructions:\n",
        "1. For each sentence, use the internal `check_importance(sentence)` tool.\n",
        "2. The tool's output is 'important' if the sentence is central to the main idea, otherwise it is 'not_important'.\n",
        "3. List the sentences that result in an 'important' output.\n",
        "4. If no sentences are important, return an empty list.\n",
        "5. Indices are 1-based.\n",
        "6. Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "x2jkAlJfThO0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**scoring-based prompting** instead gives a score to each sentence based on the relevance to the task. Also here we have the aspect-based version and the simple, importance-based one."
      ],
      "metadata": {
        "id": "8PUT3Ct-TkaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring_based_prompt(sentences, target_label):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select sentences that express the \"{target_label}\" aspect of the document.\n",
        "\n",
        "Aspect definitions:\n",
        "- challenge: problem, gap, limitation, unmet need, difficulty/motivation.\n",
        "- approach: method, model, system, algorithm, dataset design, procedure.\n",
        "- outcome: results, findings, improvements, metrics, performance, impact.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Instructions:\n",
        "1. For each sentence, assign a score from 1 (low relevance) to 5 (high relevance) for how well it expresses the \"{target_label}\" aspect.\n",
        "2. Only select sentences with a score of 4 or 5.\n",
        "3. If no sentences meet the threshold, return an empty list.\n",
        "4. Indices are 1-based.\n",
        "5. Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "8RKC2tnPTkuC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring_based_simple_prompt(sentences):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select the most important sentences from the document.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Instructions:\n",
        "1. For each sentence, assign a score from 1 (low importance) to 5 (high importance) for how central it is to the document's main idea.\n",
        "2. Only select sentences with a score of 4 or 5.\n",
        "3. If no sentences meet the threshold, return an empty list.\n",
        "4. Indices are 1-based.\n",
        "5. Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "-1yOAdzdT2ix"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**self-ask prompting** involves the model reasoning through a series of yes-no questions and using the answers to reach a conclustion."
      ],
      "metadata": {
        "id": "SNfKCbKsT5Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_ask_prompt(sentences, target_label):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select sentences that express the \"{target_label}\" aspect of the document.\n",
        "\n",
        "Aspect definitions:\n",
        "- challenge: problem, gap, limitation, unmet need, difficulty/motivation.\n",
        "- approach: method, model, system, algorithm, dataset design, procedure.\n",
        "- outcome: results, findings, improvements, metrics, performance, impact.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Instructions:\n",
        "1. Reason step-by-step. For each sentence, ask the question: \"Does this sentence primarily express the \"{target_label}\" aspect?\"\n",
        "2. Answer the question with \"Yes\" or \"No\".\n",
        "3. Compile a list of all sentences for which the answer was \"Yes\".\n",
        "4. If no sentences meet the criteria, return an empty list.\n",
        "5. Indices are 1-based.\n",
        "6. Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "RwLsmJpDT4-p"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_ask_simple_prompt(sentences):\n",
        "    numbered = [f\"Sentence {i+1}: {s}\" for i, s in enumerate(sentences)]\n",
        "    input_text = \"\\n\".join(numbered)\n",
        "\n",
        "    return f'''You are an expert in extractive summarization. Your task is to select the most important sentences from the document.\n",
        "\n",
        "Input:\n",
        "{input_text}\n",
        "\n",
        "Instructions:\n",
        "1. Reason step-by-step. First, ask the question: \"What is the main idea of this document?\"\n",
        "2. Then, for each sentence, ask: \"Does this sentence support the main idea?\"\n",
        "3. Compile a list of all sentences for which the answer was \"Yes\".\n",
        "4. If no sentences are important, return an empty list.\n",
        "5. Indices are 1-based.\n",
        "6. Return ONLY valid JSON.\n",
        "\n",
        "Return format:\n",
        "{{\"selected_sentences\": [list_of_sentence_numbers]}}'''"
      ],
      "metadata": {
        "id": "WwKcjbjqUGH6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transform gold standard to match output for each label:\n",
        "\n",
        "  so for example if the challenge_labels looks like this:\n",
        "\n",
        "  `[1,0,0,1,1,0]`\n",
        "\n",
        "  it will become\n",
        "\n",
        "  `[1, 4, 5]`\n",
        "\n"
      ],
      "metadata": {
        "id": "yrpaSqm3GR2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I want also the gold standard to match the same output\n",
        "ASPECTS = [\"challenge\", \"approach\", \"outcome\"]\n",
        "\n",
        "def labels_to_indices(labels):\n",
        "    \"\"\"0/1 list -> 1-based indices of 1s\"\"\"\n",
        "    return [i+1 for i, v in enumerate(labels) if int(v) == 1]\n",
        "\n",
        "def gold_for_doc(example):\n",
        "    \"\"\"\n",
        "    Build gold indices per aspect for ONE document.\n",
        "    Returns: {\"challenge\":[...], \"approach\":[...], \"outcome\":[...]}\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"challenge\": labels_to_indices(example[\"challenge_labels\"]),\n",
        "        \"approach\":  labels_to_indices(example[\"approach_labels\"]),\n",
        "        \"outcome\":   labels_to_indices(example[\"outcome_labels\"]),\n",
        "    }\n",
        "\n",
        "def gold_for_dataset(ds):\n",
        "    \"\"\"\n",
        "    Build gold indices per aspect for ALL docs.\n",
        "    Returns a list aligned with ds, where item i is gold_for_doc(ds[i])\n",
        "    \"\"\"\n",
        "    return [gold_for_doc(ex) for ex in ds]\n",
        "\n",
        "gold_all = gold_for_dataset(dataset)\n",
        "print(gold_all[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn-GJdXDAm6C",
        "outputId": "a5011cb7-0237-4030-a861-0bc0eac69c60"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'challenge': [1, 2, 7, 11], 'approach': [3, 15, 19, 26, 27], 'outcome': [4, 5, 20, 21, 30, 31]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "u7eLPxSj4bOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- JSON parsing that tolerates extra text ---\n",
        "def safe_extract_json(text: str):\n",
        "    text = text.strip()\n",
        "    # try direct\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # try to find {...}\n",
        "    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
        "    if m:\n",
        "        try:\n",
        "            return json.loads(m.group(0))\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}"
      ],
      "metadata": {
        "id": "kXg5h1lNAm3u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "from transformers import AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "kTjzD3P-NacX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "\n",
        "def safe_extract_json_strict(text: str):\n",
        "    \"\"\"\n",
        "    Strictly extract the model's JSON:\n",
        "      - Prefer a JSON object that contains \"selected_sentences\".\n",
        "      - Else accept a SINGLE standalone bracketed list on its own line.\n",
        "      - Otherwise return {} (no guesses; avoids capturing numbers from the prompt).\n",
        "    \"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    try:\n",
        "        js = json.loads(text)\n",
        "        if isinstance(js, dict) and \"selected_sentences\" in js:\n",
        "            return js\n",
        "        if isinstance(js, list):\n",
        "            return {\"selected_sentences\": js}\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    text_clean = re.sub(r\"^```[\\w-]*\\s*\\n\", \"\", text, flags=re.S)\n",
        "    text_clean = re.sub(r\"\\n```$\", \"\", text_clean, flags=re.S).strip()\n",
        "\n",
        "    objs = re.findall(r\"\\{[\\s\\S]*?\\}\", text_clean)\n",
        "    for s in reversed(objs):\n",
        "        try:\n",
        "            js = json.loads(s)\n",
        "            if isinstance(js, dict) and \"selected_sentences\" in js:\n",
        "                return js\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    m = re.search(r\"(?m)^\\s*\\[(?:\\s*\\d+\\s*(?:,\\s*\\d+\\s*)*)?\\]\\s*$\", text_clean)\n",
        "    if m:\n",
        "        try:\n",
        "            arr = json.loads(m.group(0))\n",
        "            return {\"selected_sentences\": arr}\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return {}\n"
      ],
      "metadata": {
        "id": "nkK0JccA9t_e"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The qwen model is pretrained on chat-like inputs, and therefore the input we have needs to be translated as a chat as well"
      ],
      "metadata": {
        "id": "Jdj1lpqjbh3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_indices_for_aspect(prompt_technique, sentences, target_label, max_new_tokens=256, show_raw=False):\n",
        "    # Build prompt and format as chat text\n",
        "    user_prompt = prompt_technique(sentences, target_label)\n",
        "    chat_text = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in extractive summarization.\"},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False\n",
        "    )\n",
        "\n",
        "    print(f'lenght= {len(tokenizer.encode(chat_text))}')\n",
        "\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Use the model's generate method directly for robust generation\n",
        "    inputs = tokenizer(chat_text, return_tensors='pt', padding=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,  # This is the only generation flag you need\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens to a string\n",
        "    out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    if show_raw:\n",
        "        print(f\"\\n[RAW OUTPUT for {target_label}]\\n{out}\\n\")\n",
        "\n",
        "    # Use the robust JSON parser on the decoded string\n",
        "    js = safe_extract_json_strict(out)\n",
        "    idxs = js.get(\"selected_sentences\", [])\n",
        "\n",
        "    # Sanitize indices\n",
        "    n = len(sentences)\n",
        "    cleaned = []\n",
        "    for v in idxs:\n",
        "        if isinstance(v, (int, float)):\n",
        "            v = int(v)\n",
        "            if 1 <= v <= n:\n",
        "                cleaned.append(v)\n",
        "    return sorted(set(cleaned))\n"
      ],
      "metadata": {
        "id": "gy_HikkzAm1i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(gold_for_doc, predicted_indices):\n",
        "\n",
        "    # Convert lists to sets for efficient intersection and difference operations\n",
        "    gold_set = set(gold_for_doc)\n",
        "    predicted_set = set(predicted_indices)\n",
        "\n",
        "    # Calculate True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
        "    true_positives = len(gold_set.intersection(predicted_set))\n",
        "    false_positives = len(predicted_set.difference(gold_set))\n",
        "    false_negatives = len(gold_set.difference(predicted_set))\n",
        "\n",
        "    # Calculate Precision\n",
        "    if true_positives + false_positives == 0:\n",
        "        precision = 0.0\n",
        "    else:\n",
        "        precision = true_positives / (true_positives + false_positives)\n",
        "\n",
        "    # Calculate Recall\n",
        "    if true_positives + false_negatives == 0:\n",
        "        recall = 0.0\n",
        "    else:\n",
        "        recall = true_positives / (true_positives + false_negatives)\n",
        "\n",
        "    # Calculate F1-score\n",
        "    if precision + recall == 0:\n",
        "        f1_score = 0.0\n",
        "    else:\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    }"
      ],
      "metadata": {
        "id": "SyHTfBeV6KzD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation loop:\n",
        "\n",
        "The following function evaluates precision, recall and f1 for each aspect both for a single document and in an aggregate way"
      ],
      "metadata": {
        "id": "knIZFHvmQ28g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation(dataset, prompt_technique, gold_all, aspects, start=0, stop=None):\n",
        "    if stop is None:\n",
        "        stop = len(dataset)\n",
        "    stop = min(stop, len(dataset))\n",
        "\n",
        "    # Dictionaries to store metrics for macro-averaging\n",
        "    all_metrics = {aspect: {\"precision\": [], \"recall\": [], \"f1_score\": []} for aspect in aspects}\n",
        "\n",
        "    for doc_idx in range(start, stop):\n",
        "        ex = dataset[doc_idx]\n",
        "        sentences = ex[\"source_sentences\"]\n",
        "        gold_standard = gold_all[doc_idx] # given that gold_all = gold_for_dataset(dataset)\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Doc {doc_idx} (id={ex.get('id', 'NA')}) | #sentences={len(sentences)}\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "        for aspect in aspects:\n",
        "            # get predicted indices for the current aspect\n",
        "            predicted_indices = predict_indices_for_aspect(\n",
        "                prompt_technique=prompt_technique,\n",
        "                sentences=sentences,\n",
        "                target_label=aspect,\n",
        "            )\n",
        "\n",
        "            # get gold standard for each aspect\n",
        "            gold_indices = gold_standard.get(aspect, [])\n",
        "\n",
        "            # 3. Calculate metrics\n",
        "            metrics = calculate_metrics(gold_indices, predicted_indices)\n",
        "\n",
        "            # Print per-document results\n",
        "            print(f\"  -> {aspect.capitalize():9s}\")\n",
        "            print(f\"     Predicted: {sorted(predicted_indices)}\")\n",
        "            print(f\"     Gold:      {sorted(gold_indices)}\")\n",
        "            print(f\"     Metrics: P={metrics['precision']:.2f}, R={metrics['recall']:.2f}, F1={metrics['f1_score']:.2f}\")\n",
        "            print()\n",
        "\n",
        "            # 4. Store metrics for aggregation\n",
        "            all_metrics[aspect][\"precision\"].append(metrics['precision'])\n",
        "            all_metrics[aspect][\"recall\"].append(metrics['recall'])\n",
        "            all_metrics[aspect][\"f1_score\"].append(metrics['f1_score'])\n",
        "\n",
        "    # After the loop, calculate and print the final aggregate scores\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"--- Final Aggregate Metrics ---\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for aspect in aspects:\n",
        "        avg_p = sum(all_metrics[aspect][\"precision\"]) / len(all_metrics[aspect][\"precision\"]) if all_metrics[aspect][\"precision\"] else 0\n",
        "        avg_r = sum(all_metrics[aspect][\"recall\"]) / len(all_metrics[aspect][\"recall\"]) if all_metrics[aspect][\"recall\"] else 0\n",
        "        avg_f1 = sum(all_metrics[aspect][\"f1_score\"]) / len(all_metrics[aspect][\"f1_score\"]) if all_metrics[aspect][\"f1_score\"] else 0\n",
        "\n",
        "        print(f\"  -> {aspect.capitalize()} Average:\")\n",
        "        print(f\"     Precision: {avg_p:.2f}\")\n",
        "        print(f\"     Recall:    {avg_r:.2f}\")\n",
        "        print(f\"     F1-score:  {avg_f1:.2f}\")\n",
        "        print()\n",
        "\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "vDD8Hos46Kvs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions for the first three documents for the **vanilla prompt**"
      ],
      "metadata": {
        "id": "QN_6b_XQa0fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_evaluation(dataset, vanilla_prompt, gold_all, ASPECTS, stop=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXNS4E6e6Kty",
        "outputId": "5bba8b39-da4e-4224-f489-6036d4539d7b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Doc 0 (id=E09-1056) | #sentences=44\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [2, 6, 11, 13, 14, 37]\n",
            "     Gold:      [1, 2, 7, 11]\n",
            "     Metrics: P=0.33, R=0.50, F1=0.40\n",
            "\n",
            "lenght= 1553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [3, 9, 19, 26, 27, 28, 29, 30, 32, 35, 36, 37, 40, 41, 42, 43]\n",
            "     Gold:      [3, 15, 19, 26, 27]\n",
            "     Metrics: P=0.25, R=0.80, F1=0.38\n",
            "\n",
            "lenght= 1551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [3, 5, 19, 20, 21, 30, 31, 32, 33, 34, 35, 39]\n",
            "     Gold:      [4, 5, 20, 21, 30, 31]\n",
            "     Metrics: P=0.42, R=0.83, F1=0.56\n",
            "\n",
            "================================================================================\n",
            "Doc 1 (id=N19-1362) | #sentences=32\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 7, 9, 12, 13]\n",
            "     Gold:      [1, 7, 9]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 3, 4, 10, 11, 14, 15, 17, 18, 24, 25, 26]\n",
            "     Gold:      [2, 3, 10, 11, 14, 15, 24, 25]\n",
            "     Metrics: P=0.67, R=1.00, F1=0.80\n",
            "\n",
            "lenght= 1594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [5, 6, 20, 21, 22, 26]\n",
            "     Gold:      [5, 6, 13, 20, 21, 22, 26]\n",
            "     Metrics: P=1.00, R=0.86, F1=0.92\n",
            "\n",
            "================================================================================\n",
            "Doc 2 (id=P01-1040) | #sentences=17\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 5, 6, 16]\n",
            "     Gold:      [1, 5, 6]\n",
            "     Metrics: P=0.75, R=1.00, F1=0.86\n",
            "\n",
            "lenght= 984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 7, 8, 12, 14]\n",
            "     Gold:      [2, 7, 14]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 982\n",
            "  -> Outcome  \n",
            "     Predicted: [9, 12, 17]\n",
            "     Gold:      [3, 4, 12, 13]\n",
            "     Metrics: P=0.33, R=0.25, F1=0.29\n",
            "\n",
            "\n",
            "================================================================================\n",
            "--- Final Aggregate Metrics ---\n",
            "================================================================================\n",
            "  -> Challenge Average:\n",
            "     Precision: 0.56\n",
            "     Recall:    0.83\n",
            "     F1-score:  0.67\n",
            "\n",
            "  -> Approach Average:\n",
            "     Precision: 0.51\n",
            "     Recall:    0.93\n",
            "     F1-score:  0.64\n",
            "\n",
            "  -> Outcome Average:\n",
            "     Precision: 0.58\n",
            "     Recall:    0.65\n",
            "     F1-score:  0.59\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions for the first three documents for the **least to most prompt**"
      ],
      "metadata": {
        "id": "99H3fi5ja9Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# least_to_most_prompt\n",
        "run_evaluation(dataset, least_to_most_prompt, gold_all, ASPECTS, stop=3)"
      ],
      "metadata": {
        "id": "v-3GVhOW6Krz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21641c4a-795d-4aa1-c92a-2f2484136f7c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Doc 0 (id=E09-1056) | #sentences=44\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [2, 6, 11, 13, 14, 37]\n",
            "     Gold:      [1, 2, 7, 11]\n",
            "     Metrics: P=0.33, R=0.50, F1=0.40\n",
            "\n",
            "lenght= 1570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [3, 9, 19, 26, 27, 28, 29, 30, 32, 35]\n",
            "     Gold:      [3, 15, 19, 26, 27]\n",
            "     Metrics: P=0.40, R=0.80, F1=0.53\n",
            "\n",
            "lenght= 1568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [3, 5, 19, 20, 21, 30, 31, 32, 33, 34, 35, 39]\n",
            "     Gold:      [4, 5, 20, 21, 30, 31]\n",
            "     Metrics: P=0.42, R=0.83, F1=0.56\n",
            "\n",
            "================================================================================\n",
            "Doc 1 (id=N19-1362) | #sentences=32\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 7, 9, 12, 13]\n",
            "     Gold:      [1, 7, 9]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 3, 4, 10, 11, 14, 15, 17, 18, 24, 25, 26]\n",
            "     Gold:      [2, 3, 10, 11, 14, 15, 24, 25]\n",
            "     Metrics: P=0.67, R=1.00, F1=0.80\n",
            "\n",
            "lenght= 1611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [5, 6, 20, 21, 22, 26]\n",
            "     Gold:      [5, 6, 13, 20, 21, 22, 26]\n",
            "     Metrics: P=1.00, R=0.86, F1=0.92\n",
            "\n",
            "================================================================================\n",
            "Doc 2 (id=P01-1040) | #sentences=17\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 5, 6, 16]\n",
            "     Gold:      [1, 5, 6]\n",
            "     Metrics: P=0.75, R=1.00, F1=0.86\n",
            "\n",
            "lenght= 1001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 7, 8, 12, 14]\n",
            "     Gold:      [2, 7, 14]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 999\n",
            "  -> Outcome  \n",
            "     Predicted: [9, 12, 13, 17]\n",
            "     Gold:      [3, 4, 12, 13]\n",
            "     Metrics: P=0.50, R=0.50, F1=0.50\n",
            "\n",
            "\n",
            "================================================================================\n",
            "--- Final Aggregate Metrics ---\n",
            "================================================================================\n",
            "  -> Challenge Average:\n",
            "     Precision: 0.56\n",
            "     Recall:    0.83\n",
            "     F1-score:  0.67\n",
            "\n",
            "  -> Approach Average:\n",
            "     Precision: 0.56\n",
            "     Recall:    0.93\n",
            "     F1-score:  0.69\n",
            "\n",
            "  -> Outcome Average:\n",
            "     Precision: 0.64\n",
            "     Recall:    0.73\n",
            "     F1-score:  0.66\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions for the first three documents for the **tool augmented prompt**"
      ],
      "metadata": {
        "id": "__ChOx19b0b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tool_augmented_prompt\n",
        "run_evaluation(dataset, tool_augmented_prompt, gold_all, ASPECTS, stop=3)"
      ],
      "metadata": {
        "id": "7L9dUV886Kpy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a055ee0e-86e7-4596-e47a-7fa6b783aef9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Doc 0 (id=E09-1056) | #sentences=44\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [2, 6, 11, 13, 14, 37]\n",
            "     Gold:      [1, 2, 7, 11]\n",
            "     Metrics: P=0.33, R=0.50, F1=0.40\n",
            "\n",
            "lenght= 1602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [3, 9, 19, 26, 27, 28, 29, 30, 32, 35, 36, 37, 40, 41, 42, 43]\n",
            "     Gold:      [3, 15, 19, 26, 27]\n",
            "     Metrics: P=0.25, R=0.80, F1=0.38\n",
            "\n",
            "lenght= 1600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [3, 5, 30, 31, 32, 33, 34, 35, 39]\n",
            "     Gold:      [4, 5, 20, 21, 30, 31]\n",
            "     Metrics: P=0.33, R=0.50, F1=0.40\n",
            "\n",
            "================================================================================\n",
            "Doc 1 (id=N19-1362) | #sentences=32\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 7, 9, 12, 13]\n",
            "     Gold:      [1, 7, 9]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 3, 4, 10, 11, 14, 15, 17, 18, 24, 25, 26]\n",
            "     Gold:      [2, 3, 10, 11, 14, 15, 24, 25]\n",
            "     Metrics: P=0.67, R=1.00, F1=0.80\n",
            "\n",
            "lenght= 1643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [5, 6, 20, 21, 22, 26]\n",
            "     Gold:      [5, 6, 13, 20, 21, 22, 26]\n",
            "     Metrics: P=1.00, R=0.86, F1=0.92\n",
            "\n",
            "================================================================================\n",
            "Doc 2 (id=P01-1040) | #sentences=17\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 5, 6]\n",
            "     Gold:      [1, 5, 6]\n",
            "     Metrics: P=1.00, R=1.00, F1=1.00\n",
            "\n",
            "lenght= 1033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 7, 8, 12, 14]\n",
            "     Gold:      [2, 7, 14]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1031\n",
            "  -> Outcome  \n",
            "     Predicted: [9, 12, 17]\n",
            "     Gold:      [3, 4, 12, 13]\n",
            "     Metrics: P=0.33, R=0.25, F1=0.29\n",
            "\n",
            "\n",
            "================================================================================\n",
            "--- Final Aggregate Metrics ---\n",
            "================================================================================\n",
            "  -> Challenge Average:\n",
            "     Precision: 0.64\n",
            "     Recall:    0.83\n",
            "     F1-score:  0.72\n",
            "\n",
            "  -> Approach Average:\n",
            "     Precision: 0.51\n",
            "     Recall:    0.93\n",
            "     F1-score:  0.64\n",
            "\n",
            "  -> Outcome Average:\n",
            "     Precision: 0.56\n",
            "     Recall:    0.54\n",
            "     F1-score:  0.54\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions for the first three documents for the **scoring based prompt**"
      ],
      "metadata": {
        "id": "_77j3xIbb-VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scoring_based_prompt\n",
        "run_evaluation(dataset, scoring_based_prompt, gold_all, ASPECTS, stop=3)"
      ],
      "metadata": {
        "id": "U8c68dMC6Knr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdb7b56-cf07-4acf-ae59-9a6f3bd0edd3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Doc 0 (id=E09-1056) | #sentences=44\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [2, 6, 11, 13, 14, 37]\n",
            "     Gold:      [1, 2, 7, 11]\n",
            "     Metrics: P=0.33, R=0.50, F1=0.40\n",
            "\n",
            "lenght= 1595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [3, 9, 19, 26, 27, 28, 29, 30, 32, 35, 36, 37, 40, 41, 42, 43]\n",
            "     Gold:      [3, 15, 19, 26, 27]\n",
            "     Metrics: P=0.25, R=0.80, F1=0.38\n",
            "\n",
            "lenght= 1593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [3, 5, 19, 20, 21, 30, 31, 32, 33, 34, 35, 39]\n",
            "     Gold:      [4, 5, 20, 21, 30, 31]\n",
            "     Metrics: P=0.42, R=0.83, F1=0.56\n",
            "\n",
            "================================================================================\n",
            "Doc 1 (id=N19-1362) | #sentences=32\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 7, 9, 13, 26]\n",
            "     Gold:      [1, 7, 9]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 3, 4, 10, 11, 14, 15, 17, 18, 24, 25, 26]\n",
            "     Gold:      [2, 3, 10, 11, 14, 15, 24, 25]\n",
            "     Metrics: P=0.67, R=1.00, F1=0.80\n",
            "\n",
            "lenght= 1636\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [5, 6, 20, 21, 22, 26]\n",
            "     Gold:      [5, 6, 13, 20, 21, 22, 26]\n",
            "     Metrics: P=1.00, R=0.86, F1=0.92\n",
            "\n",
            "================================================================================\n",
            "Doc 2 (id=P01-1040) | #sentences=17\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 5, 6, 7]\n",
            "     Gold:      [1, 5, 6]\n",
            "     Metrics: P=0.75, R=1.00, F1=0.86\n",
            "\n",
            "lenght= 1026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 7, 8, 12, 14]\n",
            "     Gold:      [2, 7, 14]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1024\n",
            "  -> Outcome  \n",
            "     Predicted: [9, 12, 17]\n",
            "     Gold:      [3, 4, 12, 13]\n",
            "     Metrics: P=0.33, R=0.25, F1=0.29\n",
            "\n",
            "\n",
            "================================================================================\n",
            "--- Final Aggregate Metrics ---\n",
            "================================================================================\n",
            "  -> Challenge Average:\n",
            "     Precision: 0.56\n",
            "     Recall:    0.83\n",
            "     F1-score:  0.67\n",
            "\n",
            "  -> Approach Average:\n",
            "     Precision: 0.51\n",
            "     Recall:    0.93\n",
            "     F1-score:  0.64\n",
            "\n",
            "  -> Outcome Average:\n",
            "     Precision: 0.58\n",
            "     Recall:    0.65\n",
            "     F1-score:  0.59\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictions for the first three documents for the **self ask prompt**"
      ],
      "metadata": {
        "id": "GBcnAjVFcNA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# self_ask_prompt\n",
        "run_evaluation(dataset, self_ask_prompt, gold_all, ASPECTS, stop=3)"
      ],
      "metadata": {
        "id": "WAc7OReR6KlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf87939d-a738-4fba-b234-77fba22bb3e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Doc 0 (id=E09-1056) | #sentences=44\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [2, 6, 11, 13, 14, 37]\n",
            "     Gold:      [1, 2, 7, 11]\n",
            "     Metrics: P=0.33, R=0.50, F1=0.40\n",
            "\n",
            "lenght= 1602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [3, 9, 19, 26, 27, 28, 29, 30, 32, 35, 36, 37, 40, 41, 42, 43]\n",
            "     Gold:      [3, 15, 19, 26, 27]\n",
            "     Metrics: P=0.25, R=0.80, F1=0.38\n",
            "\n",
            "lenght= 1600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [3, 5, 30, 31, 32, 33, 34, 35, 39]\n",
            "     Gold:      [4, 5, 20, 21, 30, 31]\n",
            "     Metrics: P=0.33, R=0.50, F1=0.40\n",
            "\n",
            "================================================================================\n",
            "Doc 1 (id=N19-1362) | #sentences=32\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 7, 9, 12, 13]\n",
            "     Gold:      [1, 7, 9]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 3, 4, 10, 11, 14, 15, 17, 18, 24, 25, 26]\n",
            "     Gold:      [2, 3, 10, 11, 14, 15, 24, 25]\n",
            "     Metrics: P=0.67, R=1.00, F1=0.80\n",
            "\n",
            "lenght= 1643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Outcome  \n",
            "     Predicted: [5, 6, 20, 21, 22, 26]\n",
            "     Gold:      [5, 6, 13, 20, 21, 22, 26]\n",
            "     Metrics: P=1.00, R=0.86, F1=0.92\n",
            "\n",
            "================================================================================\n",
            "Doc 2 (id=P01-1040) | #sentences=17\n",
            "--------------------------------------------------------------------------------\n",
            "lenght= 1031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Challenge\n",
            "     Predicted: [1, 5, 6]\n",
            "     Gold:      [1, 5, 6]\n",
            "     Metrics: P=1.00, R=1.00, F1=1.00\n",
            "\n",
            "lenght= 1033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  -> Approach \n",
            "     Predicted: [2, 7, 8, 12, 14]\n",
            "     Gold:      [2, 7, 14]\n",
            "     Metrics: P=0.60, R=1.00, F1=0.75\n",
            "\n",
            "lenght= 1031\n",
            "  -> Outcome  \n",
            "     Predicted: [9, 12, 17]\n",
            "     Gold:      [3, 4, 12, 13]\n",
            "     Metrics: P=0.33, R=0.25, F1=0.29\n",
            "\n",
            "\n",
            "================================================================================\n",
            "--- Final Aggregate Metrics ---\n",
            "================================================================================\n",
            "  -> Challenge Average:\n",
            "     Precision: 0.64\n",
            "     Recall:    0.83\n",
            "     F1-score:  0.72\n",
            "\n",
            "  -> Approach Average:\n",
            "     Precision: 0.51\n",
            "     Recall:    0.93\n",
            "     F1-score:  0.64\n",
            "\n",
            "  -> Outcome Average:\n",
            "     Precision: 0.56\n",
            "     Recall:    0.54\n",
            "     F1-score:  0.54\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4E-ONCO6Ki7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4ALWnaR6Kgt"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OXSPrNb6Ken"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZLeWJJe6KcZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yEh5BLzU6KaL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUSw1lDG6KX-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_P7j4GUf6KVp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1QmsYCcAmk3"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf3da6d406db453f913ab5076d62a7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9075aed72864d2a93988e1a58cf29b2",
              "IPY_MODEL_b3cf12bd9c0b428c9fd82df2d4b095bb",
              "IPY_MODEL_d74258f39d1d4d8f8fdc6b0e460366c7"
            ],
            "layout": "IPY_MODEL_f1c95ae78c6743e784bce69dfceb68ed"
          }
        },
        "e9075aed72864d2a93988e1a58cf29b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10b1fc4b5e33413cb9a92403d2f4ab21",
            "placeholder": "​",
            "style": "IPY_MODEL_d5a1955c2bc141ff90e82385610ddc5f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b3cf12bd9c0b428c9fd82df2d4b095bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59d9c4ad53c34f5eb091ca65bfe4cbb2",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa0122927b394e2c88dd25d35ced70bc",
            "value": 3
          }
        },
        "d74258f39d1d4d8f8fdc6b0e460366c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c300e4991c094f3ea577c5ba439a8e12",
            "placeholder": "​",
            "style": "IPY_MODEL_fafe44f7e4e04b8ba92c5cd23ada09ce",
            "value": " 3/3 [00:49&lt;00:00, 13.43s/it]"
          }
        },
        "f1c95ae78c6743e784bce69dfceb68ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b1fc4b5e33413cb9a92403d2f4ab21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a1955c2bc141ff90e82385610ddc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d9c4ad53c34f5eb091ca65bfe4cbb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa0122927b394e2c88dd25d35ced70bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c300e4991c094f3ea577c5ba439a8e12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fafe44f7e4e04b8ba92c5cd23ada09ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}