{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8d5ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/federicomarchi/Desktop/BigData&TextMining/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/federicomarchi/Desktop/BigData&TextMining/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91f39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_abstractive = load_dataset(\"sobamchan/aclsum\", \"abstractive\", split=\"test\")\n",
    "dataset_extractive = load_dataset(\"sobamchan/aclsum\", \"extractive\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34b98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_abstractive_temp = dataset_abstractive[:5]\n",
    "dataset_extractive_temp = dataset_extractive[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b94ab",
   "metadata": {},
   "source": [
    "Let's import **ROUGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9ff572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install evaluate absl-py nltk rouge-score\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "013ba117",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32e167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibilities for rouge parameter are:  \"rouge1\", \"rouge2\", \"rougeL\"\n",
    "def rouge_score(candidate, reference, rougex):\n",
    "    result = rouge.compute(\n",
    "        predictions=[candidate],\n",
    "        references=[reference],\n",
    "        rouge_types=[rougex]\n",
    "    )\n",
    "    r1 = result[rougex]\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e895b",
   "metadata": {},
   "source": [
    "### Let's define funcitons for **Summary-level's** strategies\n",
    "#### **Greedy Search**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90073c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_extractive_summary(sentences, abstractive_summaries, eval_criteria, max_sent):\n",
    "    selected = []\n",
    "    remaining = sentences[:]\n",
    "    #mask = [0] * len(sentences)\n",
    "\n",
    "    while remaining and max_sent != 0: \n",
    "        best_sentence = None\n",
    "        best_score = -1\n",
    "        for sent in remaining:\n",
    "            \n",
    "            candidate_summary = \" \".join(selected + [sent])\n",
    "            r1 = rouge_score(candidate_summary, abstractive_summaries, eval_criteria)\n",
    "            if r1 > best_score:\n",
    "                best_score = r1\n",
    "                best_sentence = sent\n",
    "       \n",
    "        selected.append(best_sentence)\n",
    "        idx = sentences.index(best_sentence)\n",
    "        #mask[idx] = 1\n",
    "        remaining.remove(best_sentence)\n",
    "        max_sent -= 1\n",
    "    \n",
    "    #return selected, mask\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca6f8a",
   "metadata": {},
   "source": [
    "#### **Beam Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1390631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_extractive_summary(sentences, abstractive_summary, eval_criteria, max_sent, beam_size=4, with_score=False):\n",
    "    # Tuple of two items:\n",
    "    #   -   selected sentences array\n",
    "    #   -   ROUGE score\n",
    "    beams = [([], 0.0)]\n",
    "    \n",
    "    for _ in range(min(max_sent, len(sentences))):\n",
    "        new_beams = []\n",
    "        for selected, _ in beams:\n",
    "            remaining = [s for s in sentences if s not in selected]\n",
    "            \n",
    "            for sent in remaining:\n",
    "                candidate_summary = \" \".join(selected + [sent])\n",
    "                r1 = rouge_score(candidate_summary, abstractive_summary, eval_criteria)\n",
    "                \n",
    "                new_beams.append((selected + [sent], r1))\n",
    "        if not new_beams:\n",
    "            break\n",
    "        # Sort by third value (ROUGE F1 score), highest score first\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        # Takes only the first beam_size elements\n",
    "        beams = new_beams[:beam_size]\n",
    "    if with_score:\n",
    "        return beams \n",
    "    return [beam for beam, _ in beams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e98931",
   "metadata": {},
   "source": [
    "### Let's define funcitons for **Sentence-level's** strategies\n",
    "#### **Local Scorer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "216998e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_extractive_summary(sentences, abstractive_summaries, eval_criteria, max_sent):\n",
    "    def rank_sentences():\n",
    "        sent_with_score = []\n",
    "        for sent in sentences:\n",
    "            r1 = rouge_score(sent, abstractive_summaries, eval_criteria)\n",
    "            sent_with_score.append({\"sentence\": sent, \"r_score\": r1})\n",
    "        sent_with_score.sort(key=lambda x: x[\"r_score\"], reverse=True)\n",
    "        return [sent[\"sentence\"] for sent in sent_with_score]\n",
    "\n",
    "    sorted_sentences = rank_sentences()\n",
    "    return sorted_sentences[:max_sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5296158",
   "metadata": {},
   "source": [
    "#### **Global Scorer**\n",
    "As heuristic in order to find the best candidates we will use **Beam Search** heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bba5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_extractive_summary(sentences, abstractive_summary, eval_criteria, max_sent, beam_size=4):\n",
    "    candidates = beam_extractive_summary(sentences, abstractive_summary, eval_criteria, max_sent, beam_size, True)\n",
    "    unique_sent_in_candidates = list(set([sent for candidate in [sents for sents, _ in candidates] for sent in candidate]))\n",
    "    unique_sent_with_scores = []\n",
    "    \n",
    "    for sent in unique_sent_in_candidates:\n",
    "        final_score = 0\n",
    "        for cand, score in candidates:\n",
    "            if sent in cand:\n",
    "                final_score += score\n",
    "        unique_sent_with_scores.append((sent, final_score))\n",
    "    \n",
    "    sorted_unique_sent_with_scores = sorted(unique_sent_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_sentences = [sent for sent, _ in sorted_unique_sent_with_scores][:max_sent]\n",
    "\n",
    "    return best_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74262737",
   "metadata": {},
   "source": [
    "### Let's now define a function to iterate over all documents\n",
    "It will also keep track of the **time** taken from each algorithm in order to compute all the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53578a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "046e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_heuristic_to_dataset(heuristic_fn, docs_sentences, abstractive_summaries, eval_criteria, max_sent=6, beam_size=0):\n",
    "    selected_list = []\n",
    "    start_time = time.perf_counter()\n",
    "    for sentences, abs_summary in zip(docs_sentences, abstractive_summaries):\n",
    "        if beam_size > 0:\n",
    "            selected = heuristic_fn(sentences, abs_summary, eval_criteria, max_sent, beam_size)\n",
    "        else:\n",
    "            selected = heuristic_fn(sentences, abs_summary, eval_criteria, max_sent)\n",
    "            \n",
    "        selected_list.append(selected)\n",
    "\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    return selected_list, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba00a4",
   "metadata": {},
   "source": [
    "### Let's first apply each heuristic once for each summary (**Challenge**, **Approach**, **Outcome**) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b9693",
   "metadata": {},
   "source": [
    "As evaluation criteria we will first use **ROUGE F-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3848289",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_criteria = \"rouge1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68745775",
   "metadata": {},
   "source": [
    "Since we want to grid search in the interval [1, 32]Â in order to determine which value for **max_sent** is the best, we will use 32. We will then use a beam size of 4 to avoid huge complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbfea526",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentences = 32\n",
    "beam_sie = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595424d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the list of all source sentences from extractive dataset\n",
    "#list_source_sentences = list(dataset_extractive[\"source_sentences\"])\n",
    "list_source_sentences = list(dataset_extractive_temp[\"source_sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c38b2",
   "metadata": {},
   "source": [
    "Let's create a dictionary to keep track of all the results we will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89d74d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"challenge\": {\n",
    "        \"greedy_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"beam_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"local_score\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"global_score\":{\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "    },\n",
    "    \"approach\": {\n",
    "        \"greedy_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"beam_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"local_score\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"global_score\":{\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "    },\n",
    "    \"outcome\": {\n",
    "        \"greedy_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"beam_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"local_score\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"global_score\":{\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "    },\n",
    "    \"all_together\": {\n",
    "        \"greedy_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"beam_search\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"local_score\": {\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "        \"global_score\":{\n",
    "            \"result\": [],\n",
    "            \"time\": None\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4941356",
   "metadata": {},
   "source": [
    "#### **Challenge** summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6f3a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the list of all challenge sentences from abstractive dataset\n",
    "#list_challenge_summaries = list(dataset_abstractive[\"challenge\"])\n",
    "list_challenge_summaries = list(dataset_abstractive_temp[\"challenge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "746f4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search\n",
    "results[\"challenge\"][\"greedy_search\"][\"result\"], results[\"challenge\"][\"greedy_search\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    greedy_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4041b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search\n",
    "results[\"challenge\"][\"beam_search\"][\"result\"], results[\"challenge\"][\"beam_search\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    beam_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences, beam_sie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "73d7dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Scorer\n",
    "results[\"challenge\"][\"local_score\"][\"result\"], results[\"challenge\"][\"local_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    local_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1934cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Scorer\n",
    "results[\"challenge\"][\"global_score\"][\"result\"], results[\"challenge\"][\"global_score\"][\"time\"]  = apply_heuristic_to_dataset(\n",
    "    global_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences, beam_sie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3499d0",
   "metadata": {},
   "source": [
    "#### **Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67d4d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_approach_summaries = list(dataset_abstractive_temp[\"approach\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cc59c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search\n",
    "results[\"approach\"][\"global_score\"][\"result\"], results[\"approach\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    greedy_extractive_summary, list_source_sentences, list_approach_summaries, evaluation_criteria, max_sentences)\n",
    "\n",
    "# Beam Search\n",
    "results[\"approach\"][\"global_score\"][\"result\"], results[\"approach\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    beam_extractive_summary, list_source_sentences, list_approach_summaries, evaluation_criteria, max_sentences, beam_sie)\n",
    "\n",
    "# Local Score\n",
    "results[\"approach\"][\"global_score\"][\"result\"], results[\"approach\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    local_extractive_summary, list_source_sentences, list_approach_summaries, evaluation_criteria, max_sentences)\n",
    "\n",
    "# Global Score\n",
    "results[\"approach\"][\"global_score\"][\"result\"], results[\"approach\"][\"global_score\"][\"time\"]  = apply_heuristic_to_dataset(\n",
    "    global_extractive_summary, list_source_sentences, list_approach_summaries, evaluation_criteria, max_sentences, beam_sie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd893e",
   "metadata": {},
   "source": [
    "#### **Outcome**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bea3c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_outcome_summaries = list(dataset_abstractive_temp[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9831c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search\n",
    "results[\"outcome\"][\"global_score\"][\"result\"], results[\"outcome\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    greedy_extractive_summary, list_source_sentences, list_outcome_summaries, evaluation_criteria, max_sentences)\n",
    "\n",
    "# Beam Search\n",
    "results[\"outcome\"][\"global_score\"][\"result\"], results[\"outcome\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    beam_extractive_summary, list_source_sentences, list_outcome_summaries, evaluation_criteria, max_sentences, beam_sie)\n",
    "\n",
    "# Local Score\n",
    "results[\"outcome\"][\"global_score\"][\"result\"], results[\"outcome\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    local_extractive_summary, list_source_sentences, list_outcome_summaries, evaluation_criteria, max_sentences)\n",
    "\n",
    "# Global Score\n",
    "results[\"outcome\"][\"global_score\"][\"result\"], results[\"outcome\"][\"global_score\"][\"time\"]  = apply_heuristic_to_dataset(\n",
    "    global_extractive_summary, list_source_sentences, list_outcome_summaries, evaluation_criteria, max_sentences, beam_sie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7088ae",
   "metadata": {},
   "source": [
    "#### **All Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b6dae4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_summaries = [\n",
    "    [c, a, o] \n",
    "    for c, a, o in zip(\n",
    "        dataset_abstractive_temp[\"challenge\"], \n",
    "        dataset_abstractive_temp[\"approach\"], \n",
    "        dataset_abstractive_temp[\"outcome\"]\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search\n",
    "results[\"all_together\"][\"global_score\"][\"result\"], results[\"all_together\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    greedy_extractive_summary, list_source_sentences, list_all_summaries, evaluation_criteria, max_sentences)\n",
    "\n",
    "# Beam Search\n",
    "results[\"all_together\"][\"global_score\"][\"result\"], results[\"all_together\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    beam_extractive_summary, list_source_sentences, list_all_summaries, evaluation_criteria, max_sentences, beam_sie)\n",
    "\n",
    "# Local Score\n",
    "results[\"all_together\"][\"global_score\"][\"result\"], results[\"all_together\"][\"global_score\"][\"time\"] = apply_heuristic_to_dataset(\n",
    "    local_extractive_summary, list_source_sentences, list_all_summaries, evaluation_criteria, max_sentences)\n",
    "\n",
    "# Global Score\n",
    "results[\"all_together\"][\"global_score\"][\"result\"], results[\"all_together\"][\"global_score\"][\"time\"]  = apply_heuristic_to_dataset(\n",
    "    global_extractive_summary, list_source_sentences, list_outcome_summaries, evaluation_criteria, max_sentences, beam_sie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0fc8bd",
   "metadata": {},
   "source": [
    "#### Grid Search to define best K \n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aad9ab",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53a548",
   "metadata": {},
   "source": [
    "Let's make a function which, for each document, concatenates the sentences we have extracted from the selected list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_concatenation(sentence_list):\n",
    "    merged_sentences = [\" \".join(inner_list) for inner_list in sentence_list]\n",
    "    return merged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29960bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Generated summaries concatenation\n",
    "greedy_list_summaries_challenge_conc = sentence_concatenation(greedy_list_summaries_challenge)\n",
    "beam_list_summaries_challenge_conc = sentence_concatenation(beam_list_summaries_challenge)\n",
    "local_list_summaries_challenge_conc = sentence_concatenation(local_list_summaries_challenge)\n",
    "global_list_summaries_challenge_conc = sentence_concatenation(global_list_summaries_challenge)\n",
    "\n",
    "# Labels concatenation\n",
    "#labels_challenge = sentence_concatenation(dataset_extractive[\"challenge_labels\"])\n",
    "labels_challenge = sentence_concatenation(dataset_extractive_temp[\"challenge_sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098c9f6",
   "metadata": {},
   "source": [
    "We now need the function in order to evaluate the average performances over the whole test set, base on **ROUGE F1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ff9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_performance(predicted_summaries, label_summaries, eval_criteria):\n",
    "    # In case something went wrong\n",
    "    if len(predicted_summaries) != len(label_summaries):\n",
    "        return None\n",
    "    \n",
    "    r1_sum = 0\n",
    "    for pred, label in zip(predicted_summaries, label_summaries):\n",
    "        r1 = rouge_score(pred, label, eval_criteria)\n",
    "        r1_sum += r1\n",
    "    \n",
    "    return r1_sum / len(predicted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a12b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average ROUGE F-1 score for Greedy Search heuristic is: 0.00183\n",
      "Time taken: 69.5s\n"
     ]
    }
   ],
   "source": [
    "r1_greedy_challenge = avg_performance(greedy_list_summaries_challenge_conc, labels_challenge, evaluation_criteria)\n",
    "print(f'The average ROUGE F-1 score for Greedy Search heuristic is: {round(r1_greedy_challenge, 5)}')\n",
    "print(f'Time taken: {round(greedy_challenge_time, 1)}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
