{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa8d5ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b91f39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_abstractive = load_dataset(\"sobamchan/aclsum\", \"abstractive\", split=\"test\")\n",
    "dataset_extractive = load_dataset(\"sobamchan/aclsum\", \"extractive\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e34b98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_abstractive_10 = dataset_abstractive[:10]\n",
    "dataset_extractive_10 = dataset_extractive[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b94ab",
   "metadata": {},
   "source": [
    "Let's import **ROUGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d9ff572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install evaluate absl-py nltk rouge-score\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "013ba117",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f32e167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possibilities for rouge parameter are:  \"rouge1\", \"rouge2\", \"rougeL\"\n",
    "def rouge_score(candidate, reference, rougex):\n",
    "    result = rouge.compute(\n",
    "        predictions=[candidate],\n",
    "        references=[reference],\n",
    "        rouge_types=[rougex]\n",
    "    )\n",
    "    r1 = result[rougex]\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e895b",
   "metadata": {},
   "source": [
    "### Let's define funcitons for **Summary-level's** strategies\n",
    "#### **Greedy Search**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90073c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_extractive_summary(sentences, abstractive_summaries, eval_criteria, max_sent):\n",
    "    selected = []\n",
    "    remaining = sentences[:]\n",
    "    #mask = [0] * len(sentences)\n",
    "\n",
    "    while remaining and max_sent != 0: \n",
    "        best_sentence = None\n",
    "        best_score = -1\n",
    "        for sent in remaining:\n",
    "            \n",
    "            candidate_summary = \" \".join(selected + [sent])\n",
    "            r1 = rouge_score(candidate_summary, abstractive_summaries, eval_criteria)\n",
    "            if r1 > best_score:\n",
    "                best_score = r1\n",
    "                best_sentence = sent\n",
    "       \n",
    "        selected.append(best_sentence)\n",
    "        idx = sentences.index(best_sentence)\n",
    "        #mask[idx] = 1\n",
    "        remaining.remove(best_sentence)\n",
    "        max_sent -= 1\n",
    "    \n",
    "    #return selected, mask\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca6f8a",
   "metadata": {},
   "source": [
    "#### **Beam Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1390631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_extractive_summary(sentences, abstractive_summary, eval_criteria, max_sent, beam_size=4,):\n",
    "    # Tuple of two items:\n",
    "    #   -   selected sentences array\n",
    "    #   -   ROUGE score\n",
    "    beams = [([], 0.0)]\n",
    "    \n",
    "    for _ in range(max_sent):\n",
    "        new_beams = []\n",
    "        for selected, _ in beams:\n",
    "            remaining = [s for s in sentences if s not in selected]\n",
    "            \n",
    "            for sent in remaining:\n",
    "                candidate_summary = \" \".join(selected + [sent])\n",
    "                r1 = rouge_score(candidate_summary, abstractive_summary, eval_criteria)\n",
    "                \n",
    "                new_beams.append((selected + [sent], r1))\n",
    "        \n",
    "        # Sort by third value (ROUGE F1 score), highest score first\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        # Takes only the first beam_size elements\n",
    "        beams = new_beams[:beam_size]\n",
    "    \n",
    "    return [beam for beam, _ in beams]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e98931",
   "metadata": {},
   "source": [
    "### Let's define funcitons for **Sentence-level's** strategies\n",
    "#### **Local Scorer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "216998e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_extractive_summary(sentences, abstractive_summaries, eval_criteria, max_sent):\n",
    "    def rank_sentences():\n",
    "        sent_with_score = []\n",
    "        for sent in sentences:\n",
    "            r1 = rouge_score(sent, abstractive_summaries, eval_criteria)\n",
    "            sent_with_score.append({\"sentence\": sent, \"r_score\": r1})\n",
    "        sent_with_score.sort(key=lambda x: x[\"r_score\"], reverse=True)\n",
    "        return [sent[\"sentence\"] for sent in sent_with_score]\n",
    "\n",
    "    sorted_sentences = rank_sentences()\n",
    "    return sorted_sentences[:max_sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5296158",
   "metadata": {},
   "source": [
    "#### **Global Scorer**\n",
    "As heuristic in order to find the best candidates we will use **Beam Search** heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2bba5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_extractive_summary(sentences, abstractive_summary, eval_criteria, max_sent, beam_size=4):\n",
    "    candidates = beam_extractive_summary(sentences, abstractive_summary, eval_criteria, max_sent, beam_size)\n",
    "    unique_sent_in_candidates = list(set([sent for candidate in [beam[0] for beam in candidates] for sent in candidate]))\n",
    "    unique_sent_with_scores = []\n",
    "\n",
    "    for sent in unique_sent_in_candidates:\n",
    "        final_score = 0\n",
    "        for cand, score in candidates:\n",
    "            if sent in cand:\n",
    "                final_score += score\n",
    "        unique_sent_with_scores.append((sent, final_score))\n",
    "    \n",
    "    sorted_unique_sent_with_scores = sorted(unique_sent_with_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_sentences = [sent for sent, _ in sorted_unique_sent_with_scores][:max_sent]\n",
    "\n",
    "    return best_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74262737",
   "metadata": {},
   "source": [
    "### Let's now define a function to iterate over all documents\n",
    "It will also keep track of the **time** taken from each algorithm in order to compute all the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e53578a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "046e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_heuristic_to_dataset(heuristic_fn, docs_sentences, abstractive_summaries, eval_criteria, max_sent=6, beam_size=0):\n",
    "    selected_list = []\n",
    "    start_time = time.perf_counter()\n",
    "    for sentences, abs_summary in zip(docs_sentences, abstractive_summaries):\n",
    "        if beam_size > 0:\n",
    "            selected = heuristic_fn(sentences, abs_summary, eval_criteria, max_sent, beam_size)\n",
    "        else:\n",
    "            selected = heuristic_fn(sentences, abs_summary, eval_criteria, max_sent)\n",
    "            \n",
    "        selected_list.append(selected)\n",
    "\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "    return selected_list, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba00a4",
   "metadata": {},
   "source": [
    "### Let's first apply each heuristic once for each summary (**Challenge**, **Approach**, **Outcome**) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b9693",
   "metadata": {},
   "source": [
    "As evaluation criteria we will first use **ROUGE F-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3848289",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_criteria = \"rouge1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68745775",
   "metadata": {},
   "source": [
    "Since we want to grid search in the interval [1, 32] in order to determine which value for **max_sent** is the best, we will use 32. We will then use a beam size of 4 to avoid huge complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbfea526",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentences = 32\n",
    "beam_sie = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "595424d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the list of all source sentences from extractive dataset\n",
    "#list_source_sentences = list(dataset_extractive[\"source_sentences\"])\n",
    "list_source_sentences = list(dataset_extractive_10[\"source_sentences\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4941356",
   "metadata": {},
   "source": [
    "#### **Challenge** summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6f3a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the list of all challenge sentences from abstractive dataset\n",
    "#list_challenge_summaries = list(dataset_abstractive[\"challenge\"])\n",
    "list_challenge_summaries = list(dataset_abstractive_10[\"challenge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Search\n",
    "greedy_list_summaries_challenge, greedy_challenge_time = apply_heuristic_to_dataset(greedy_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search\n",
    "beam_list_summaries_challenge, beam_challenge_time = apply_heuristic_to_dataset(beam_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences, beam_sie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Scorer\n",
    "local_list_summaries_challenge, local_challenge_time = apply_heuristic_to_dataset(local_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Scorer\n",
    "global_list_summaries_challenge, global_challenge_time  = apply_heuristic_to_dataset(global_extractive_summary, list_source_sentences, list_challenge_summaries, evaluation_criteria, max_sentences, beam_sie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aad9ab",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53a548",
   "metadata": {},
   "source": [
    "Let's make a function which, for each document, concatenates the sentences we have extracted from the selected list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_concatenation(sentence_list):\n",
    "    merged_sentences = [\" \".join(inner_list) for inner_list in sentence_list]\n",
    "    return merged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff76800e",
   "metadata": {},
   "source": [
    "#### THE TWO CELLS BELOW ARE **TEMPORARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_list_summaries_challenge = beam_list_summaries_challenge[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b557a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_list_summaries_challenge = [beam for beam, _ in beam_list_summaries_challenge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29960bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated summaries concatenation\n",
    "greedy_list_summaries_challenge_conc = sentence_concatenation(greedy_list_summaries_challenge)\n",
    "beam_list_summaries_challenge_conc = sentence_concatenation(beam_list_summaries_challenge)\n",
    "local_list_summaries_challenge_conc = sentence_concatenation(local_list_summaries_challenge)\n",
    "global_list_summaries_challenge_conc = sentence_concatenation(global_list_summaries_challenge)\n",
    "\n",
    "# Labels concatenation\n",
    "#labels_challenge = sentence_concatenation(dataset_extractive[\"challenge_labels\"])\n",
    "labels_challenge = sentence_concatenation(dataset_extractive_10[\"challenge_sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098c9f6",
   "metadata": {},
   "source": [
    "We now need the function in order to evaluate the average performances over the whole test set, base on **ROUGE F1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ff9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_performance(predicted_summaries, label_summaries, eval_criteria):\n",
    "    # In case something went wrong\n",
    "    if len(predicted_summaries) != len(label_summaries):\n",
    "        return None\n",
    "    \n",
    "    r1_sum = 0\n",
    "    for pred, label in zip(predicted_summaries, label_summaries):\n",
    "        r1 = rouge_score(pred, label, eval_criteria)\n",
    "        r1_sum += r1\n",
    "    \n",
    "    return r1_sum / len(predicted_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a12b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average ROUGE F-1 score for Greedy Search heuristic is: 0.00183\n",
      "Time taken: 69.5s\n"
     ]
    }
   ],
   "source": [
    "r1_greedy_challenge = avg_performance(greedy_list_summaries_challenge_conc, labels_challenge, evaluation_criteria)\n",
    "print(f'The average ROUGE F-1 score for Greedy Search heuristic is: {round(r1_greedy_challenge, 5)}')\n",
    "print(f'Time taken: {round(greedy_challenge_time, 1)}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
